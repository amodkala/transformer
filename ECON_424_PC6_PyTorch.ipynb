{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amodkala/transformer/blob/main/ECON_424_PC6_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ixpOGWw-oNp",
        "outputId": "ab9413c5-be1b-4a79-c2cb-20d28eab65e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# fix for torch.compile() error on A100 runtime\n",
        "\n",
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO8Ae4tlgXJQ"
      },
      "outputs": [],
      "source": [
        "# Imports, 'global' variables, and class definitions\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import os.path\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from torchtext.data import get_tokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.transforms import VocabTransform\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, classification_report, confusion_matrix\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "gpu_ok = device.type != \"cpu\" and torch.cuda.get_device_capability() in ((7, 0), (8, 0), (9, 0))\n",
        "if gpu_ok and \"A100\" in torch.cuda.get_device_name(0):\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=200, device=None):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.encoding = self.encoding.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.encoding[:, :x.size(1)].detach()\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, embeddings, d_model, nhead, num_encoder_layers, num_classes, dropout=0.0, transformer_dropout=0.0):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=True)\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, device=device)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_model, transformer_dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.embedding(src) * math.sqrt(d_model)\n",
        "        src = self.embedding_dropout(src)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src)\n",
        "        output = output.mean(dim=1)\n",
        "        # output = self.output_dropout(output)\n",
        "        return self.fc(output)\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab, max_len=200):\n",
        "        assert len(texts) == len(labels)\n",
        "        self.labels = labels\n",
        "        self.texts = [torch.tensor([vocab[token] for token in text], dtype=torch.long) for text in texts]\n",
        "        self.max_len = max_len if max_len is not None else max(len(t) for t in self.texts)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        if len(text) < self.max_len:\n",
        "            # Pad the sequence if it's shorter than max_len\n",
        "            text = F.pad(text, (0, self.max_len - len(text)), 'constant', vocab['<pad>'])\n",
        "        return text, self.labels[idx]\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        texts, labels = zip(*batch)\n",
        "        # If all texts are already padded to max_len, no need for dynamic padding\n",
        "        return torch.stack(texts), torch.tensor(labels, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RviwsNa7BMSq",
        "outputId": "bf996852-9e76-4bd3-b590-b7cda890f063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-14 18:29:51--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-12-14 18:29:52--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-12-14 18:29:52--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2023-12-14 18:32:32 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AlyixZcNJoe"
      },
      "outputs": [],
      "source": [
        "# implement preprocessing from https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/reports/6880837.pdf\n",
        "#\n",
        "# tokenize pros and cons columns\n",
        "# limit them to 100 words in length\n",
        "# combine them, add padding characters for consistent 200 length\n",
        "\n",
        "# Load CSV\n",
        "size = \"small\"\n",
        "filename = f\"Econ424_F2023_PC6_glassdoor_training_{size}_v1.csv\"\n",
        "\n",
        "df = pd.read_csv(filename, low_memory=False)\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "specials = {\n",
        "    \"unknown\": \"<unk>\",\n",
        "    \"padding\": \"<pad>\",\n",
        "}\n",
        "\n",
        "df[\"pros\"] = df[\"pros\"].apply(lambda x: tokenizer(str(x))[:100])\n",
        "df[\"cons\"] = df[\"cons\"].apply(lambda x: tokenizer(str(x))[:100])\n",
        "\n",
        "df[\"text\"] = df[\"pros\"] + df[\"cons\"]\n",
        "df[\"text\"].apply(lambda x: x.extend([specials[\"padding\"]] * (200 - len(x))))\n",
        "\n",
        "# 5 features\n",
        "df[\"pros_tokens\"] = df[\"pros\"].apply(lambda x: len(x))\n",
        "df[\"cons_tokens\"] = df[\"cons\"].apply(lambda x: len(x))\n",
        "df[\"text_tokens\"] = df[\"pros_tokens\"] + df[\"cons_tokens\"]\n",
        "df[\"headline_tokens\"] = df[\"headline\"].apply(lambda x: len(tokenizer(str(x))))\n",
        "df[\"total_tokens\"] = df[\"headline_tokens\"] + df[\"text_tokens\"]\n",
        "\n",
        "vocab = build_vocab_from_iterator((tokens for tokens in df[\"text\"]), specials=list(specials.values()), max_tokens=10000)\n",
        "vocab.set_default_index(vocab[specials[\"unknown\"]])  # Set default index for unknown tokens\n",
        "\n",
        "# basic integer encoding\n",
        "#\n",
        "# vt = VocabTransform(vocab)\n",
        "# print(df[\"text\"][0])\n",
        "# print(vt(df[\"text\"][0]))\n",
        "\n",
        "# GloVe embeddings\n",
        "embedding_dim = 300 # or whatever dimension your GloVe embeddings are\n",
        "vocab_size = len(vocab)  # Assuming 'vocab' is your vocabulary built from the dataset\n",
        "\n",
        "embeddings_dict = {}\n",
        "glove_size = \"6B\"\n",
        "glove_dim = \"300d\"\n",
        "with open(f\"glove.{glove_size}.{glove_dim}.txt\", 'r', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings_dict[word] = vector\n",
        "\n",
        "# Initialize the embedding matrix\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, idx in vocab.get_stoi().items():\n",
        "    embedding_vector = embeddings_dict.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "    else:\n",
        "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n",
        "\n",
        "pad_index = vocab[specials[\"padding\"]]  # Get the index of the padding token\n",
        "embedding_matrix[pad_index] = np.zeros((embedding_dim, ))  # Set the padding token's embedding to a zero vector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_ratings = df.groupby('firm')['overall_rating'].mean().reset_index()\n",
        "average_ratings = average_ratings.rename(columns={'overall_rating': 'average_rating'})\n",
        "\n",
        "# Step 2: Merge the average ratings back into the original DataFrame\n",
        "df = pd.merge(df, average_ratings, on='firm', how='left')\n"
      ],
      "metadata": {
        "id": "CCF3F92ky9Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_threshold = 3  # Ratings below this are considered low\n",
        "high_threshold = 4  # Ratings above this are considered high\n",
        "\n",
        "# Create three new DataFrames\n",
        "low_ratings_df = df[df['average_rating'] < low_threshold]\n",
        "medium_ratings_df = df[(df['average_rating'] >= low_threshold) & (df['average_rating'] <= high_threshold)]\n",
        "high_ratings_df = df[df['average_rating'] > high_threshold]\n",
        "\n",
        "print(len(low_ratings_df))\n",
        "print(len(medium_ratings_df))\n",
        "print(len(high_ratings_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6W9Aryj3z016",
        "outputId": "61871c99-dcd9-419c-ce9b-96aa0c5b068c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4313\n",
            "96479\n",
            "9937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def get_unique_top_terms(df, column, exclude_terms, num_terms=10):\n",
        "    # Flatten the list of words in the DataFrame column and filter out stopwords and excluded terms\n",
        "    all_words = [word for sublist in df[column] for word in sublist if word not in stop_words and word not in exclude_terms and word.isalpha()]\n",
        "    return Counter(all_words).most_common(num_terms)\n",
        "\n",
        "# Assuming low_ratings_df, medium_ratings_df, high_ratings_df are your DataFrames\n",
        "# Start with the high ratings group\n",
        "high_common_terms = get_unique_top_terms(high_ratings_df, 'cons', set(), num_terms=25)\n",
        "\n",
        "# Then, calculate for medium ratings excluding high ratings' top terms\n",
        "medium_common_terms = get_unique_top_terms(medium_ratings_df, 'cons', set(term for term, _ in high_common_terms), num_terms=25)\n",
        "\n",
        "# Lastly, calculate for low ratings excluding both medium and high ratings' top terms\n",
        "low_common_terms = get_unique_top_terms(low_ratings_df, 'cons', set(term for term, _ in high_common_terms + medium_common_terms), num_terms=25)\n",
        "\n",
        "print([term for term, num in high_common_terms])\n",
        "print([term for term, num in medium_common_terms])\n",
        "print([term for term, num in low_common_terms])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdgDHjCl6RlF",
        "outputId": "db04dcf0-189e-4c7c-bc1e-8c5735242cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['work', 'company', 'management', 'hours', 'people', 'get', 'long', 'time', 'sometimes', 'much', 'balance', 'hard', 'working', 'many', 'lot', 'like', 'cons', 'good', 'life', 'really', 'career', 'difficult', 'culture', 'job', 'big']\n",
            "['pay', 'employees', 'managers', 'salary', 'low', 'poor', 'staff', 'lack', 'team', 'bad', 'high', 'little', 'growth', 'one', 'less', 'times', 'environment', 'years', 'politics', 'slow', 'always', 'even', 'need', 'new', 'make']\n",
            "['training', 'senior', 'manager', 'support', 'progression', 'customers', 'never', 'care', 'business', 'store', 'targets', 'day', 'would', 'sales', 'enough', 'pressure', 'office', 'worked', 'everything', 'communication', 'could', 'paid', 'know', 'hr', 'wage']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "def get_top_n_grams(corpus, n=None, n_grams=2):\n",
        "    vec = CountVectorizer(ngram_range=(n_grams, n_grams), stop_words='english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "# Example usage with the low ratings DataFrame\n",
        "low_ratings_corpus = [' '.join(row) for row in low_ratings_df['cons']]\n",
        "low_ratings_top_bigrams = get_top_n_grams(low_ratings_corpus, n=10, n_grams=3)\n",
        "medium_ratings_corpus = [' '.join(row) for row in medium_ratings_df['cons']]\n",
        "medium_ratings_top_bigrams = get_top_n_grams(medium_ratings_corpus, n=10, n_grams=3)\n",
        "high_ratings_corpus = [' '.join(row) for row in high_ratings_df['cons']]\n",
        "high_ratings_top_bigrams = get_top_n_grams(high_ratings_corpus, n=10, n_grams=3)\n",
        "\n",
        "print(\"Top 10 bigrams in low ratings 'cons':\", low_ratings_top_bigrams)\n",
        "print(\"Top 10 bigrams in medium ratings 'cons':\", medium_ratings_top_bigrams)\n",
        "print(\"Top 10 bigrams in high ratings 'cons':\", high_ratings_top_bigrams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2tO6hItBywv",
        "outputId": "67138b8c-abae-4214-e67f-3d3d378e8dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 bigrams in low ratings 'cons': [('work life balance', 69), ('high staff turnover', 25), ('poor work life', 15), ('work long hours', 13), ('high turnover staff', 11), ('staff turnover high', 9), ('poor management poor', 9), ('poor management lack', 8), ('good place work', 8), ('poor senior management', 8)]\n",
            "Top 10 bigrams in medium ratings 'cons': [('work life balance', 5500), ('long working hours', 776), ('poor work life', 353), ('work long hours', 274), ('hours busy season', 216), ('long work hours', 186), ('long hours busy', 169), ('long hours work', 160), ('great place work', 154), ('hours work life', 130)]\n",
            "Top 10 bigrams in high ratings 'cons': [('work life balance', 577), ('long working hours', 52), ('great place work', 32), ('work long hours', 31), ('poor work life', 28), ('long work hours', 21), ('fast paced environment', 19), ('life balance difficult', 17), ('life balance challenge', 17), ('bureaucracy bureaucracy bureaucracy', 16)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(low_ratings_df[\"cons\"].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS9UKs4O65_g",
        "outputId": "2b55e9f4-042c-4c63-a6cb-239f49e00250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "204     [poor, management, ., no, growth, opportunitie...\n",
            "466     [they, expected, so, much, ,, but, give, very,...\n",
            "482     [rigid, managerial, structure, ., strong, deli...\n",
            "1099                      [can, be, stressful, at, times]\n",
            "1104    [constant, reorganisations, ., no, apparent, s...\n",
            "Name: cons, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsZKgxiNJ3o0",
        "outputId": "cf5711a6-f6aa-4328-b6d0-7eb1d9d4583b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "absent words:  577\n",
            "matching words:  9422\n",
            "misaligned words:  0\n"
          ]
        }
      ],
      "source": [
        "# test vocab/GloVe alignment\n",
        "\n",
        "absent, matches, notmatches = 0, 0, 0\n",
        "\n",
        "for word, idx in list(vocab.get_stoi().items()):  # Check first 10 words for example\n",
        "    vocab_embedding = embedding_matrix[idx]\n",
        "    glove_embedding = embeddings_dict.get(word, \"Not in GloVe\")\n",
        "\n",
        "    if isinstance(glove_embedding, str):\n",
        "        absent = absent + 1\n",
        "    elif np.allclose(vocab_embedding, glove_embedding):\n",
        "        matches = matches + 1\n",
        "    else:\n",
        "        notmatches = notmatches + 1\n",
        "\n",
        "print(\"absent words: \", absent)\n",
        "print(\"matching words: \", matches)\n",
        "print(\"misaligned words: \", notmatches)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbtr8K4GbKi1"
      },
      "outputs": [],
      "source": [
        "# Create train + test datasets, instantiate model\n",
        "train_df, test_df = train_test_split(df, test_size=0.2)\n",
        "\n",
        "# create tensors from 0-indexed labels\n",
        "train_labels = torch.tensor(train_df['overall_rating'].values - 1)\n",
        "test_labels = torch.tensor(test_df['overall_rating'].values - 1)\n",
        "full_labels = torch.tensor(df['overall_rating'].values - 1)\n",
        "\n",
        "# Assuming you have already instantiated the dataset\n",
        "train_dataset = ReviewDataset(train_df['text'].tolist(), train_labels.tolist(), vocab)\n",
        "test_dataset = ReviewDataset(test_df['text'].tolist(), test_labels.tolist(), vocab)\n",
        "full_dataset = ReviewDataset(df['text'].tolist(), full_labels.tolist(), vocab)\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# DataLoader with collate_fn\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn, pin_memory=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=test_dataset.collate_fn, pin_memory=True)\n",
        "full_dataloader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True, collate_fn=full_dataset.collate_fn, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.groupby(\"overall_rating\").size()/len(df) * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFmms7QmsQhi",
        "outputId": "0e9e6846-8a08-471f-c81e-af6676546f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "overall_rating\n",
            "1     7.769419\n",
            "2    10.176196\n",
            "3    24.703556\n",
            "4    33.190944\n",
            "5    24.159886\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYoLBQTmgeJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "387c28fe-a278-479e-99e6-c579cbfe8516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "# Model parameters\n",
        "embeddings = embedding_matrix\n",
        "d_model = embedding_dim # Embedding dimension\n",
        "nhead = 6  # Number of heads in MultiHeadAttention\n",
        "num_encoder_layers = 4  # Number of TransformerEncoder layers\n",
        "num_classes = 5  # Number of output classes\n",
        "\n",
        "# Instantiate the model\n",
        "torch.manual_seed(0)\n",
        "model = TransformerClassifier(embeddings, d_model, nhead, num_encoder_layers, num_classes)\n",
        "model = model.to(device)\n",
        "if gpu_ok:\n",
        "    model = torch.compile(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "epochs 3, class_weights, no dropout, lr 6e-5, heads 6, layers 4, large dataset => accuracy 0.453\n",
        "\n",
        "epochs 3, class_weights, no dropout, lr 6e-5, heads 6, layers 4 => accuracy 0.378\n",
        "\n",
        "epochs 3, class_weights, no dropout, lr 5e-5, heads 6, layers 4 => accuracy 0.374\n",
        "\n",
        "epochs 3, class_weights, no dropout, lr 5e-5, heads 6, layers 3 => accuracy 0.373\n",
        "\n",
        "epochs 3, class_weights, no dropout, lr 4e-5, heads 6, layers 4 => accuracy 0.368\n",
        "\n",
        "epochs 3, class_weights, no dropout, lr 5e-5, heads 6, layers 6 => accuracy 0.367\n",
        "\n",
        "epochs 3, class_weights, dropout 0.1, lr 5e-5, heads 6, layers 4 => accuracy 0.367\n",
        "\n",
        "epochs 3, class_weights, no dropout, lr 5e-5, heads 3, layers 3 => accuracy 0.367\n",
        "\n",
        "epochs 3, class_weights, no dropout, lr 5e-5, heads 2, layers 2 => accuracy 0.36\n",
        "\n",
        "epochs 3, no class_weights, dropout 0.25, lr 1e-5 => accuracy 0.36 (didn't predict label=1)\n",
        "\n",
        "epochs 3, no class_weights, dropout 0.25, lr 1e-6 => accuracy 0.33 (didn't predict label=1)\n",
        "\n",
        "epochs 3, no class_weights, dropout 0.3, lr 1e-5 => accuracy 0.33 (didn't predict label=1,4)\n",
        "\n",
        "epochs 3, no class_weights, dropout 0.2, lr 1e-5 => accuracy 0.33 (didn't predict label=4)\n",
        "\n",
        "epochs 3, class weights, no dropout, lr 1e-5, heads 6, layers 6 => accuracy 0.29\n",
        "\n",
        "epochs 3, class_weights, dropout 0.2, lr 1e-5, heads 6, layers 6 => accuracy 0.29\n",
        "\n",
        "epochs 3, class_weights, dropout 0.2, lr 1e-6, heads 6, layers 6 => accuracy 0.25\n",
        "\n",
        "epochs 3, class_weights, dropout 0.4, lr 1e-5, heads 6, layers 6 => accuracy 0.24"
      ],
      "metadata": {
        "id": "2F8plVweadqj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0Z203nogp3Q",
        "outputId": "188d8215-930f-44e3-959a-5f0ad704b2c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/18: 100%|█████████▉| 6914/6921 [01:19<00:00, 102.99it/s, accuracy=29.27%, loss=0.0944]/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
            "  torch.has_cuda,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
            "  torch.has_cudnn,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
            "  torch.has_mps,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
            "  torch.has_mkldnn,\n",
            "Epoch 1/18: 100%|██████████| 6921/6921 [01:32<00:00, 74.66it/s, accuracy=29.27%, loss=0.0944]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 1, Loss: 1.5101, Accuracy: 29.27%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/18: 100%|██████████| 6921/6921 [01:08<00:00, 101.27it/s, accuracy=35.78%, loss=0.0877]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 2, Loss: 1.4027, Accuracy: 35.78%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/18: 100%|██████████| 6921/6921 [01:09<00:00, 99.44it/s, accuracy=38.54%, loss=0.0843] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 3, Loss: 1.3492, Accuracy: 38.54%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/18: 100%|██████████| 6921/6921 [01:08<00:00, 101.02it/s, accuracy=39.93%, loss=0.0822]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 4, Loss: 1.3151, Accuracy: 39.93%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/18: 100%|██████████| 6921/6921 [01:09<00:00, 100.24it/s, accuracy=41.01%, loss=0.0805]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 5, Loss: 1.2884, Accuracy: 41.01%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/18: 100%|██████████| 6921/6921 [01:09<00:00, 98.88it/s, accuracy=42.06%, loss=0.0792]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 6, Loss: 1.2664, Accuracy: 42.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/18: 100%|██████████| 6921/6921 [01:09<00:00, 99.38it/s, accuracy=42.66%, loss=0.078]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 7, Loss: 1.2485, Accuracy: 42.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/18: 100%|██████████| 6921/6921 [01:09<00:00, 99.42it/s, accuracy=43.35%, loss=0.0769] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 8, Loss: 1.2296, Accuracy: 43.35%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/18: 100%|██████████| 6921/6921 [01:09<00:00, 99.56it/s, accuracy=43.74%, loss=0.0759]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 9, Loss: 1.2136, Accuracy: 43.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/18: 100%|██████████| 6921/6921 [01:09<00:00, 99.63it/s, accuracy=44.22%, loss=0.0751] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 10, Loss: 1.2013, Accuracy: 44.22%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/18: 100%|██████████| 6921/6921 [01:09<00:00, 99.90it/s, accuracy=44.79%, loss=0.0742] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 11, Loss: 1.1873, Accuracy: 44.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/18: 100%|██████████| 6921/6921 [01:08<00:00, 100.40it/s, accuracy=45.10%, loss=0.0735]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 12, Loss: 1.1759, Accuracy: 45.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/18: 100%|██████████| 6921/6921 [01:10<00:00, 98.49it/s, accuracy=45.57%, loss=0.0727] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 13, Loss: 1.1625, Accuracy: 45.57%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/18: 100%|██████████| 6921/6921 [01:09<00:00, 100.10it/s, accuracy=46.00%, loss=0.0719]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 14, Loss: 1.1498, Accuracy: 46.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/18: 100%|██████████| 6921/6921 [01:09<00:00, 100.28it/s, accuracy=46.46%, loss=0.0712]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 15, Loss: 1.1396, Accuracy: 46.46%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/18: 100%|██████████| 6921/6921 [01:10<00:00, 98.49it/s, accuracy=46.87%, loss=0.0704] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 16, Loss: 1.1262, Accuracy: 46.87%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/18: 100%|██████████| 6921/6921 [01:09<00:00, 99.32it/s, accuracy=47.26%, loss=0.0697] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 17, Loss: 1.1147, Accuracy: 47.26%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/18: 100%|██████████| 6921/6921 [01:09<00:00, 99.15it/s, accuracy=47.40%, loss=0.0689]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 18, Loss: 1.1028, Accuracy: 47.40%\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "\n",
        "# Training setup (assuming you have a train function set up)\n",
        "\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels.numpy())\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "step_size = 3\n",
        "num_epochs = 6 * step_size # Number of training epochs\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=6e-5)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "model.train()\n",
        "\n",
        "# Initialize a list to store all predictions and labels\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    mode = \"predict\"\n",
        "    dl = train_dataloader if mode == \"eval\" else full_dataloader\n",
        "\n",
        "    progress_bar = tqdm(dl, desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
        "    for input, label in progress_bar:\n",
        "        input = input.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        # Forward pass, backward pass, and optimize\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input)\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "\n",
        "        # Calculate loss and accuracy\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        correct_predictions += (predicted == label).sum().item()\n",
        "        total_predictions += label.size(0)\n",
        "\n",
        "        # Update progress bar description\n",
        "        avg_loss = total_loss / total_predictions\n",
        "        accuracy = correct_predictions / total_predictions * 100\n",
        "        progress_bar.set_postfix(loss=avg_loss, accuracy=f'{accuracy:.2f}%')\n",
        "\n",
        "        # Store predictions and actual labels on GPU\n",
        "        all_predictions.append(output)\n",
        "        all_labels.append(label)\n",
        "\n",
        "    # Print average loss and accuracy for this epoch\n",
        "    avg_loss = total_loss / len(dl)\n",
        "    accuracy = correct_predictions / total_predictions * 100\n",
        "    print(f'End of Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Concatenate all predictions and labels and then move to CPU\n",
        "all_predictions = torch.cat(all_predictions).cpu()\n",
        "all_labels = torch.cat(all_labels).cpu()\n",
        "\n",
        "# If your task is a classification task, convert logits to predicted class indices\n",
        "_, all_predictions = torch.max(all_predictions, 1)\n",
        "\n",
        "# Convert tensors to numpy arrays for metric calculation\n",
        "all_predictions_np = all_predictions.numpy()\n",
        "all_labels_np = all_labels.numpy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(all_labels_np, all_predictions_np, labels=[i for i in range(5)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq1ePljEtbLo",
        "outputId": "63e4aae4-e0d2-46ac-b83c-d32539620217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 99497  28494  15681   4216   6966]\n",
            " [ 52796  66999  51719  17267  14043]\n",
            " [ 44662  78390 164935 122948  81437]\n",
            " [ 23009  40990 148983 226247 222307]\n",
            " [ 13223  11993  50804 110579 294937]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model from disk\n",
        "\n",
        "if os.path.isfile(\"model_state_dict.pth\"):\n",
        "    model.load_state_dict(torch.load('model_state_dict.pth'))\n"
      ],
      "metadata": {
        "id": "YfyhMj5cHB9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy95aSrVpOF0"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize a list to store all predictions and labels\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "# Disabling gradient calculation\n",
        "with torch.no_grad():\n",
        "    progress_bar = tqdm(test_dataloader, desc=f'Evaluating model')\n",
        "    for inputs, labels in progress_bar:\n",
        "        # Transfer inputs and labels to the same device as the model\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Store predictions and actual labels on GPU\n",
        "        all_predictions.append(outputs)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "# Concatenate all predictions and labels and then move to CPU\n",
        "all_predictions = torch.cat(all_predictions).cpu()\n",
        "all_labels = torch.cat(all_labels).cpu()\n",
        "\n",
        "# If your task is a classification task, convert logits to predicted class indices\n",
        "_, all_predictions = torch.max(all_predictions, 1)\n",
        "\n",
        "# Convert tensors to numpy arrays for metric calculation\n",
        "all_predictions_np = all_predictions.numpy()\n",
        "all_labels_np = all_labels.numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[\"prediction\"] = all_predictions_np\n",
        "correct_df = test_df[test_df[\"overall_rating\"] == test_df[\"prediction\"]]\n",
        "incorrect_df = test_df[test_df[\"overall_rating\"] != test_df[\"prediction\"]]\n",
        "\n",
        "features = ['pros_tokens', 'cons_tokens', 'text_tokens', 'headline_tokens', 'total_tokens']\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "for i, feature in enumerate(features, 1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    sns.histplot(correct_df[feature], color=\"blue\", label=\"Correct Predictions\", kde=True, stat=\"density\", linewidth=0)\n",
        "    sns.histplot(incorrect_df[feature], color=\"red\", label=\"Incorrect Predictions\", kde=True, stat=\"density\", linewidth=0)\n",
        "    plt.title(f\"Distribution of {feature}\")\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('graph5.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "0GvFdmwHsTwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "for i, feature in enumerate(['pros_tokens', 'cons_tokens', 'text_tokens', 'headline_tokens', 'total_tokens'], 1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    sns.boxplot(x='overall_rating', y=feature, data=df)\n",
        "    plt.title(f'{feature} by Overall Rating')\n",
        "    plt.xlabel('Overall Rating')\n",
        "    plt.ylabel(feature)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('graph6.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "B6KDMKAOxtG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7xuajVubSse"
      },
      "outputs": [],
      "source": [
        "# Save model to disk\n",
        "torch.save(model.state_dict(), 'model_state_dict.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Qvtu9zhY9Q_",
        "outputId": "0db4a5fa-aa67-4cd3-891b-d3c93f555641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4277786307110152\n"
          ]
        }
      ],
      "source": [
        "assert({0, 1, 2, 3, 4} == set(all_predictions_np))\n",
        "assert(len(all_predictions_np) == len(all_labels_np))\n",
        "correct = 0\n",
        "for i in range(len(all_labels_np)):\n",
        "    if all_predictions_np[i] == all_labels_np[i]:\n",
        "        correct += 1\n",
        "print(f'Accuracy: {correct/len(all_labels_np)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7fLezsiiykm",
        "outputId": "bacf0fb8-5b69-434e-918e-c954e02aba07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating model: 100%|██████████| 3561/3561 [00:10<00:00, 327.91it/s]\n"
          ]
        }
      ],
      "source": [
        "pred_df = pd.read_csv(\"/content/424_F2023_Final_PC_glassdoor_test_without_response_v1.csv\", low_memory=False)\n",
        "\n",
        "pred_df[\"pros\"] = pred_df[\"pros\"].apply(lambda x: tokenizer(str(x))[:100])\n",
        "pred_df[\"cons\"] = pred_df[\"cons\"].apply(lambda x: tokenizer(str(x))[:100])\n",
        "pred_df[\"text\"] = pred_df[\"pros\"] + pred_df[\"cons\"]\n",
        "pred_df[\"text\"].apply(lambda x: x.extend([specials[\"padding\"]] * (200 - len(x))))\n",
        "\n",
        "# Assuming test_labels are available\n",
        "# If not, you can use dummy labels as they won't be used in prediction\n",
        "pred_labels = torch.zeros(len(pred_df))  # Dummy labels\n",
        "\n",
        "pred_dataset = ReviewDataset(pred_df['text'].tolist(), pred_labels.tolist(), vocab)\n",
        "pred_dataloader = DataLoader(pred_dataset, batch_size=batch_size, shuffle=False, collate_fn=pred_dataset.collate_fn, pin_memory=True)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Store predictions\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    progress_bar = tqdm(pred_dataloader, desc=f'Evaluating model')\n",
        "    for inputs, _ in progress_bar:\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Convert outputs to probabilities and then to class labels\n",
        "        predicted_labels = torch.argmax(outputs.data, dim=1)\n",
        "\n",
        "        predictions.extend(predicted_labels.cpu().numpy())\n",
        "\n",
        "predictions = [pred + 1 for pred in predictions]\n",
        "preds_df = pd.DataFrame(predictions) # account for the 0-indexed labels from earlier\n",
        "preds_df.to_csv(\"predictions.csv\", index=False, header=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['pros_tokens', 'cons_tokens', 'text_tokens', 'headline_tokens', 'total_tokens']\n",
        "\n",
        "# graph 2\n",
        "plt.figure(figsize=(10, 8))\n",
        "corr_matrix = train_df[features].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap Among Features\")\n",
        "plt.savefig('graph2.png')  # Specify your save path here\n",
        "plt.close()\n",
        "\n",
        "# graph 3\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "for i, feature in enumerate(features, 1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    sns.histplot(train_df[feature], color=\"blue\", label=\"Training\", kde=True, stat=\"density\", linewidth=0)\n",
        "    sns.histplot(test_df[feature], color=\"red\", label=\"Test\", kde=True, stat=\"density\", linewidth=0)\n",
        "    plt.title(f\"Distribution of {feature}\")\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('graph3.png')  # Specify your save path here\n",
        "plt.close()\n",
        "\n",
        "# graph 4\n",
        "train_preds = all_predictions.tolist()\n",
        "train_labels = all_labels.tolist()\n",
        "predictions = predictions\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.histplot(train_preds, color=\"green\", label=\"Train Predictions\", kde=True, stat=\"density\", linewidth=0)\n",
        "sns.histplot(train_labels, color=\"blue\", label=\"Actual Train Values\", kde=True, stat=\"density\", linewidth=0)\n",
        "sns.histplot(predictions, color=\"red\", label=\"Test Predictions\", kde=True, stat=\"density\", linewidth=0)\n",
        "plt.title(\"Distribution of Predictions and Actual Values\")\n",
        "plt.legend()\n",
        "plt.savefig('graph4.png')  # Specify your save path here\n",
        "plt.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "JggKAanqfR0x"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMu+oV0vNS0uUE/Q9rh54cD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}